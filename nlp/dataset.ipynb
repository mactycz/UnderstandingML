{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts\n",
    "For this particular model, I will use some ancient greek philosophy, from Marcus Aurelius, Epictetus, Seneca and Cicero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\n",
    "    'https://www.gutenberg.org/cache/epub/2680/pg2680.txt', #meditations\n",
    "    'https://www.gutenberg.org/cache/epub/10661/pg10661.txt', #epictetus\n",
    "    'https://www.gutenberg.org/cache/epub/56075/pg56075.txt', #seneca1\n",
    "    'https://www.gutenberg.org/cache/epub/3794/pg3794.txt', #seneca2\n",
    "    'https://www.gutenberg.org/cache/epub/14988/pg14988.txt', #cicero1\n",
    "    'https://www.gutenberg.org/cache/epub/58418/pg58418.txt', #cicero2\n",
    "    'https://www.gutenberg.org/cache/epub/50692/pg50692.txt'  #cicero3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r\"\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\\s*(.*)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    cur_text=requests.get(url).content.decode('utf-8')\n",
    "    cur_text=re.search(pattern, cur_text, re.DOTALL) # avoid the beginning of each gutenberg ebook\n",
    "    full_text+=cur_text.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4458638"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After combining text for all of the texts, it is now time for tokenization.\n",
    "Tokenization is inspired by the video of the great Andrej Karpathy: https://www.youtube.com/watch?v=zduSFxRajkE&t=154s&ab_channel=AndrejKarpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[]\n",
    "vocab_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(256): #first 256 unicodes characters for vocab\n",
    "    vocab.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_tokens={}\n",
    "tokens_to_ids={}\n",
    "id_to_tokens[0] = '<UNK>'\n",
    "tokens_to_ids['<UNK>'] = 0\n",
    "for i in range(1,256):\n",
    "    id_to_tokens[i]=chr(i)\n",
    "    tokens_to_ids[chr(i)]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = []\n",
    "for char in full_text:\n",
    "    char_code = ord(char)\n",
    "    if char_code < 256:\n",
    "        tokenized_text.append(char_code)\n",
    "    else:\n",
    "        tokenized_text.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import gc\n",
    "def get_tokens(tokenized_text,vocab_size,id_to_tokens,tokens_to_ids):\n",
    "    print(len(id_to_tokens))\n",
    "    if vocab_size>len(id_to_tokens):\n",
    "        pairs=Counter()\n",
    "        for i in range(len(tokenized_text)-1):\n",
    "            pairs[(tokenized_text[i],tokenized_text[i+1])]+=1\n",
    "\n",
    "        if not pairs:\n",
    "            return tokenized_text, id_to_tokens, tokens_to_ids\n",
    "        most_common = pairs.most_common(1)[0][0]\n",
    "        del pairs\n",
    "        gc.collect() # need to force this garbage collector, as the memory might get crazy\n",
    "        new_idx = len(id_to_tokens)\n",
    "        new_token = id_to_tokens[most_common[0]]+id_to_tokens[most_common[1]]\n",
    "        id_to_tokens[new_idx]=new_token\n",
    "        tokens_to_ids[new_token]=new_idx\n",
    "        i=0\n",
    "        new_tokens=[]\n",
    "        while i<len(tokenized_text):\n",
    "            if i < len(tokenized_text) - 1 and  (tokenized_text[i],tokenized_text[i+1])==most_common:\n",
    "                new_tokens.append(new_idx)\n",
    "                i+=2\n",
    "            else: \n",
    "                new_tokens.append(tokenized_text[i])\n",
    "                i+=1\n",
    "        print(len(new_tokens))\n",
    "        return get_tokens(new_tokens,vocab_size,id_to_tokens,tokens_to_ids)\n",
    "    \n",
    "    else: return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "4345252\n",
      "257\n",
      "4255418\n",
      "258\n",
      "4172982\n",
      "259\n",
      "4102707\n",
      "260\n",
      "4034957\n",
      "261\n",
      "3973254\n",
      "262\n",
      "3944364\n",
      "263\n",
      "3891153\n",
      "264\n",
      "3838971\n",
      "265\n",
      "3787062\n",
      "266\n",
      "3737085\n",
      "267\n",
      "3700103\n",
      "268\n",
      "3663682\n",
      "269\n",
      "3628121\n",
      "270\n",
      "3595409\n",
      "271\n",
      "3563887\n",
      "272\n",
      "3532605\n",
      "273\n",
      "3504382\n",
      "274\n",
      "3480151\n",
      "275\n",
      "3456705\n",
      "276\n",
      "3433600\n",
      "277\n",
      "3421200\n",
      "278\n",
      "3399386\n",
      "279\n",
      "3377989\n",
      "280\n",
      "3357245\n",
      "281\n",
      "3336633\n",
      "282\n",
      "3316241\n",
      "283\n",
      "3297078\n",
      "284\n",
      "3278463\n",
      "285\n",
      "3260060\n",
      "286\n",
      "3242191\n",
      "287\n",
      "3224432\n",
      "288\n",
      "3207797\n",
      "289\n",
      "3191181\n",
      "290\n",
      "3174594\n",
      "291\n",
      "3158412\n",
      "292\n",
      "3143510\n",
      "293\n",
      "3128916\n",
      "294\n",
      "3114792\n",
      "295\n",
      "3101032\n",
      "296\n",
      "3089160\n",
      "297\n",
      "3075622\n",
      "298\n",
      "3062113\n",
      "299\n",
      "3049447\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "new_tokenized_tex = get_tokens(tokenized_text,vocab_size,id_to_tokens,tokens_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDITATIONS\n",
      "\n",
      "By Marcus Aurelius\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "\n",
      "     NOTES\n",
      "\n",
      "     INTRODUCTION\n",
      "\n",
      "     FIRST BOOK\n",
      "\n",
      "     SECOND BOOK\n",
      "\n",
      "     THIRD BOO\n"
     ]
    }
   ],
   "source": [
    "p=''\n",
    "for i in new_tokenized_tex[:100]:\n",
    "    p+=id_to_tokens[i]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution works! But the recursion seem a bit messy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
