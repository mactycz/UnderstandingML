{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts\n",
    "For this particular model, I will use some ancient greek philosophy, from Marcus Aurelius, Epictetus, Seneca and Cicero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\n",
    "    'https://www.gutenberg.org/cache/epub/2680/pg2680.txt', #meditations\n",
    "    'https://www.gutenberg.org/cache/epub/10661/pg10661.txt', #epictetus\n",
    "    'https://www.gutenberg.org/cache/epub/56075/pg56075.txt', #seneca1\n",
    "    'https://www.gutenberg.org/cache/epub/3794/pg3794.txt', #seneca2\n",
    "    'https://www.gutenberg.org/cache/epub/14988/pg14988.txt', #cicero1\n",
    "    'https://www.gutenberg.org/cache/epub/58418/pg58418.txt', #cicero2\n",
    "    'https://www.gutenberg.org/cache/epub/50692/pg50692.txt'  #cicero3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r\"\\*\\*\\* START OF THE PROJECT GUTENBERG EBOOK.*?\\*\\*\\*\\s*(.*)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    cur_text=requests.get(url).content.decode('utf-8')\n",
    "    cur_text=re.search(pattern, cur_text, re.DOTALL) # avoid the beginning of each gutenberg ebook\n",
    "    full_text+=cur_text.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4458638"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After combining text for all of the texts, it is now time for tokenization.\n",
    "Tokenization is inspired by the video of the great Andrej Karpathy: https://www.youtube.com/watch?v=zduSFxRajkE&t=154s&ab_channel=AndrejKarpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[]\n",
    "vocab_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(256): #first 256 unicodes characters for vocab\n",
    "    vocab.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_tokens={}\n",
    "tokens_to_ids={}\n",
    "id_to_tokens[0] = '<UNK>'\n",
    "tokens_to_ids['<UNK>'] = 0\n",
    "for i in range(1,256):\n",
    "    id_to_tokens[i]=chr(i)\n",
    "    tokens_to_ids[chr(i)]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = []\n",
    "for char in full_text:\n",
    "    char_code = ord(char)\n",
    "    if char_code < 256:\n",
    "        tokenized_text.append(char_code)\n",
    "    else:\n",
    "        tokenized_text.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import gc\n",
    "def get_tokens(tokenized_text,vocab_size,id_to_tokens,tokens_to_ids):\n",
    "    if vocab_size>len(id_to_tokens):\n",
    "        pairs=Counter()\n",
    "        for i in range(len(tokenized_text)-1):\n",
    "            pairs[(tokenized_text[i],tokenized_text[i+1])]+=1\n",
    "\n",
    "        if not pairs:\n",
    "            return tokenized_text, id_to_tokens, tokens_to_ids\n",
    "        most_common = pairs.most_common(1)[0][0]\n",
    "        del pairs\n",
    "        gc.collect() # need to force this garbage collector, as the memory might get crazy\n",
    "        new_idx = len(id_to_tokens)\n",
    "        new_token = id_to_tokens[most_common[0]]+id_to_tokens[most_common[1]]\n",
    "        id_to_tokens[new_idx]=new_token\n",
    "        tokens_to_ids[new_token]=new_idx\n",
    "        i=0\n",
    "        new_tokens=[]\n",
    "        while i<len(tokenized_text):\n",
    "            if i < len(tokenized_text) - 1 and  (tokenized_text[i],tokenized_text[i+1])==most_common:\n",
    "                new_tokens.append(new_idx)\n",
    "                i+=2\n",
    "            else: \n",
    "                new_tokens.append(tokenized_text[i])\n",
    "                i+=1\n",
    "        return get_tokens(new_tokens,vocab_size,id_to_tokens,tokens_to_ids)\n",
    "    \n",
    "    else: return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenized_tex = get_tokens(tokenized_text,vocab_size,id_to_tokens,tokens_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDITATIONS\n",
      "\n",
      "By Marcus Aurelius\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "\n",
      "     NOTES\n",
      "\n",
      "     INTRODUCTION\n",
      "\n",
      "     FIR\n"
     ]
    }
   ],
   "source": [
    "p=''\n",
    "for i in new_tokenized_tex[:100]:\n",
    "    p+=id_to_tokens[i]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution works! But the recursion seems a bit messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import gc\n",
    "def get_tokens_v2(tokenized_text,vocab_size,id_to_tokens,tokens_to_ids):\n",
    "    for i in range(vocab_size-len(id_to_tokens)):\n",
    "        pairs=Counter()\n",
    "        for j in range(len(tokenized_text)-1):\n",
    "            pairs[(tokenized_text[j],tokenized_text[j+1])]+=1\n",
    "\n",
    "        if not pairs:\n",
    "            return tokenized_text, id_to_tokens, tokens_to_ids\n",
    "        most_common = pairs.most_common(1)[0][0]\n",
    "        del pairs\n",
    "        gc.collect() # need to force this garbage collector, as the memory might get crazy\n",
    "        new_idx = len(id_to_tokens)\n",
    "        new_token = id_to_tokens[most_common[0]]+id_to_tokens[most_common[1]]\n",
    "        id_to_tokens[new_idx]=new_token\n",
    "        tokens_to_ids[new_token]=new_idx\n",
    "        j=0\n",
    "        new_tokens=[]\n",
    "        while j<len(tokenized_text):\n",
    "            if j < len(tokenized_text) - 1 and  (tokenized_text[j],tokenized_text[j+1])==most_common:\n",
    "                new_tokens.append(new_idx)\n",
    "                j+=2\n",
    "            else: \n",
    "                new_tokens.append(tokenized_text[j])\n",
    "                j+=1\n",
    "        tokenized_text = new_tokens\n",
    "    return tokenized_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenized_text = get_tokens_v2(tokenized_text,vocab_size,id_to_tokens,tokens_to_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEDITATIONS\n",
      "\n",
      "By Marcus Aurelius\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CONTENTS\n",
      "\n",
      "\n",
      "     NOTES\n",
      "\n",
      "     INTRODUCTION\n",
      "\n",
      "     FIRST BOOK\n",
      "\n",
      "     SECOND BOOK\n",
      "\n",
      "     THIRD BOO\n"
     ]
    }
   ],
   "source": [
    "p=''\n",
    "for i in new_tokenized_text[:100]:\n",
    "    p+=id_to_tokens[i]\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is a bit clearer and slightly faster!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
